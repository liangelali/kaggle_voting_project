{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras import optimizers\n",
    "from sklearn.svm import SVC\n",
    "from keras.layers import BatchNormalization\n",
    "import csv\n",
    "\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "training_data = load_data('train_2008.csv', 1)\n",
    "np.random.shuffle(training_data)\n",
    "# get all of the parameters\n",
    "train_data = training_data[:,:-1]\n",
    "# get all the targets\n",
    "train_target = training_data[:, -1]\n",
    "\n",
    "# load testing data\n",
    "test_data_08 = load_data('test_2008.csv', 1)\n",
    "test_data_12 = load_data('test_2012.csv', 1)\n",
    "\n",
    "# find any column which contains only one value in all its rows (such as interview month/year)\n",
    "useless_columns = np.all(train_data == train_data[0,:], axis=0)\n",
    "indices_to_remove = []\n",
    "for i in range(len(useless_columns)):\n",
    "    if useless_columns[i]:\n",
    "        indices_to_remove.append(i)\n",
    "        \n",
    "#delete those columns\n",
    "train_data = np.delete(train_data, indices_to_remove, 1)\n",
    "test_data_08 = np.delete(test_data_08, indices_to_remove, 1)\n",
    "test_data_12 = np.delete(test_data_12, indices_to_remove, 1)\n",
    "\n",
    "# train_target = np.reshape(training_data[:, -1], (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6354.274281844813, 2001.0085936420126, 1769.558765044904, 1740.0016910066265, 1591.5415341456326, 1512.8712783768926, 1367.2313990321109, 1363.064708408233, 1361.5278545849462, 1270.6635529152434, 1208.6145868555109, 979.8610848576337, 925.6815819690952, 874.2389835343976, 841.0199437722187] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "# get top k features\n",
    "num_params = 15\n",
    "test = SelectKBest(k=num_params)\n",
    "fit = test.fit(train_data, train_target)\n",
    "\n",
    "scores = fit.scores_.tolist()\n",
    "scores_copy = fit.scores_.tolist()\n",
    "best_scores = []\n",
    "best_indices = []\n",
    "\n",
    "for i in range(num_params):\n",
    "    m = max(scores)\n",
    "    best_scores.append(m)\n",
    "    best_indices.append(scores_copy.index(m))\n",
    "    scores.remove(m)\n",
    "print(best_scores, best_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:, best_indices]\n",
    "test_data_08 = test_data_08[:, best_indices]\n",
    "test_data_12 = test_data_12[:, best_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41386 samples, validate on 10347 samples\n",
      "Epoch 1/10\n",
      "41386/41386 [==============================] - 7s 179us/step - loss: 0.6140 - val_loss: 0.5279\n",
      "Epoch 2/10\n",
      "41386/41386 [==============================] - 3s 78us/step - loss: 0.5621 - val_loss: 0.5081\n",
      "Epoch 3/10\n",
      "41386/41386 [==============================] - 3s 75us/step - loss: 0.5320 - val_loss: 0.4983\n",
      "Epoch 4/10\n",
      "41386/41386 [==============================] - 3s 77us/step - loss: 0.5152 - val_loss: 0.4931\n",
      "Epoch 5/10\n",
      "41386/41386 [==============================] - 3s 79us/step - loss: 0.5042 - val_loss: 0.4869\n",
      "Epoch 6/10\n",
      "41386/41386 [==============================] - 3s 70us/step - loss: 0.4986 - val_loss: 0.4958\n",
      "Epoch 7/10\n",
      "41386/41386 [==============================] - 3s 68us/step - loss: 0.4950 - val_loss: 0.4814\n",
      "Epoch 8/10\n",
      "41386/41386 [==============================] - 3s 75us/step - loss: 0.4940 - val_loss: 0.4807\n",
      "Epoch 9/10\n",
      "41386/41386 [==============================] - 3s 75us/step - loss: 0.4920 - val_loss: 0.4792\n",
      "Epoch 10/10\n",
      "41386/41386 [==============================] - 3s 71us/step - loss: 0.4920 - val_loss: 0.4857\n",
      "0.7546193859088351\n",
      "Train on 41386 samples, validate on 10347 samples\n",
      "Epoch 1/10\n",
      "41386/41386 [==============================] - 8s 186us/step - loss: 0.6110 - val_loss: 0.5194\n",
      "Epoch 2/10\n",
      "41386/41386 [==============================] - 3s 82us/step - loss: 0.5585 - val_loss: 0.5026\n",
      "Epoch 3/10\n",
      "41386/41386 [==============================] - 3s 80us/step - loss: 0.5298 - val_loss: 0.4961\n",
      "Epoch 4/10\n",
      "41386/41386 [==============================] - 3s 78us/step - loss: 0.5112 - val_loss: 0.5051\n",
      "Epoch 5/10\n",
      "41386/41386 [==============================] - 3s 78us/step - loss: 0.5014 - val_loss: 0.4931\n",
      "Epoch 6/10\n",
      "41386/41386 [==============================] - 3s 77us/step - loss: 0.4953 - val_loss: 0.4810\n",
      "Epoch 7/10\n",
      "41386/41386 [==============================] - 3s 74us/step - loss: 0.4915 - val_loss: 0.4855\n",
      "Epoch 8/10\n",
      "41386/41386 [==============================] - 3s 77us/step - loss: 0.4899 - val_loss: 0.4767\n",
      "Epoch 9/10\n",
      "41386/41386 [==============================] - 3s 76us/step - loss: 0.4886 - val_loss: 0.4786\n",
      "Epoch 10/10\n",
      "41386/41386 [==============================] - 3s 79us/step - loss: 0.4873 - val_loss: 0.4792\n",
      "0.7402173065802008\n",
      "Train on 41387 samples, validate on 10347 samples\n",
      "Epoch 1/10\n",
      "41387/41387 [==============================] - 8s 186us/step - loss: 0.6125 - val_loss: 0.5114\n",
      "Epoch 2/10\n",
      "41387/41387 [==============================] - 3s 75us/step - loss: 0.5606 - val_loss: 0.5070\n",
      "Epoch 3/10\n",
      "41387/41387 [==============================] - 3s 80us/step - loss: 0.5309 - val_loss: 0.4907\n",
      "Epoch 4/10\n",
      "41387/41387 [==============================] - 3s 78us/step - loss: 0.5131 - val_loss: 0.5031\n",
      "Epoch 5/10\n",
      "41387/41387 [==============================] - 3s 80us/step - loss: 0.5026 - val_loss: 0.4827\n",
      "Epoch 6/10\n",
      "41387/41387 [==============================] - 3s 76us/step - loss: 0.4972 - val_loss: 0.4788\n",
      "Epoch 7/10\n",
      "41387/41387 [==============================] - 3s 76us/step - loss: 0.4936 - val_loss: 0.4808\n",
      "Epoch 8/10\n",
      "41387/41387 [==============================] - 3s 79us/step - loss: 0.4918 - val_loss: 0.4786\n",
      "Epoch 9/10\n",
      "41387/41387 [==============================] - 3s 82us/step - loss: 0.4906 - val_loss: 0.4836\n",
      "Epoch 10/10\n",
      "41387/41387 [==============================] - 3s 82us/step - loss: 0.4901 - val_loss: 0.4826\n",
      "0.7515264281742701\n",
      "Train on 41387 samples, validate on 10347 samples\n",
      "Epoch 1/10\n",
      "41387/41387 [==============================] - 9s 207us/step - loss: 0.6126 - val_loss: 0.5056\n",
      "Epoch 2/10\n",
      "41387/41387 [==============================] - 3s 82us/step - loss: 0.5603 - val_loss: 0.5875\n",
      "Epoch 3/10\n",
      "41387/41387 [==============================] - 3s 77us/step - loss: 0.5314 - val_loss: 0.5002\n",
      "Epoch 4/10\n",
      "41387/41387 [==============================] - 3s 79us/step - loss: 0.5148 - val_loss: 0.4952\n",
      "Epoch 5/10\n",
      "41387/41387 [==============================] - 3s 78us/step - loss: 0.5054 - val_loss: 0.4964\n",
      "Epoch 6/10\n",
      "41387/41387 [==============================] - 3s 80us/step - loss: 0.4996 - val_loss: 0.4820\n",
      "Epoch 7/10\n",
      "41387/41387 [==============================] - 3s 79us/step - loss: 0.4958 - val_loss: 0.4826\n",
      "Epoch 8/10\n",
      "41387/41387 [==============================] - 3s 78us/step - loss: 0.4949 - val_loss: 0.4791\n",
      "Epoch 9/10\n",
      "41387/41387 [==============================] - 3s 75us/step - loss: 0.4934 - val_loss: 0.4767\n",
      "Epoch 10/10\n",
      "41387/41387 [==============================] - 3s 76us/step - loss: 0.4931 - val_loss: 0.4869\n",
      "0.7546760302079677\n",
      "Train on 41387 samples, validate on 10347 samples\n",
      "Epoch 1/10\n",
      "41387/41387 [==============================] - 8s 191us/step - loss: 0.6146 - val_loss: 0.5287\n",
      "Epoch 2/10\n",
      "41387/41387 [==============================] - 3s 79us/step - loss: 0.5613 - val_loss: 0.5253\n",
      "Epoch 3/10\n",
      "41387/41387 [==============================] - 3s 78us/step - loss: 0.5320 - val_loss: 0.5017\n",
      "Epoch 4/10\n",
      "41387/41387 [==============================] - 3s 78us/step - loss: 0.5151 - val_loss: 0.4912\n",
      "Epoch 5/10\n",
      "41387/41387 [==============================] - 3s 79us/step - loss: 0.5051 - val_loss: 0.4871\n",
      "Epoch 6/10\n",
      "41387/41387 [==============================] - 3s 81us/step - loss: 0.4991 - val_loss: 0.4900\n",
      "Epoch 7/10\n",
      "41387/41387 [==============================] - 3s 80us/step - loss: 0.4968 - val_loss: 0.4880\n",
      "Epoch 8/10\n",
      "41387/41387 [==============================] - 3s 82us/step - loss: 0.4949 - val_loss: 0.4882\n",
      "Epoch 9/10\n",
      "41387/41387 [==============================] - 3s 78us/step - loss: 0.4941 - val_loss: 0.4848\n",
      "Epoch 10/10\n",
      "41387/41387 [==============================] - 3s 71us/step - loss: 0.4939 - val_loss: 0.4886\n",
      "0.7651763442834025\n"
     ]
    }
   ],
   "source": [
    "# 5-fold validation\n",
    "kf = KFold(n_splits=5)\n",
    "models = []\n",
    "aucs = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_data):\n",
    "    X_train, y_train = train_data[train_index], train_target[train_index]\n",
    "    X_test, y_test = train_data[test_index], train_target[test_index]\n",
    "    \n",
    "    # test = tree.DecisionTreeClassifier(criterion='gini')\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "#     clf = SVC(gamma='auto')\n",
    "#     clf.fit(X_train, y_train) \n",
    "    \n",
    "#     pred = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(600, input_shape=(len(best_indices),), kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    #--------------------------------------------\n",
    "#     adam = optimizers.adam(lr = 0.005, decay = 0.0000001)\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(480, input_dim=X_train.shape[1],\n",
    "#                     kernel_initializer='normal',\n",
    "#                     #kernel_regularizer=regularizers.l2(0.02),\n",
    "#                     activation=\"relu\"))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(240,\n",
    "#                     #kernel_regularizer=regularizers.l2(0.02),\n",
    "#                     activation=\"tanh\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "#     #--------------------------------------------\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=150)\n",
    "    \n",
    "    #--------------------------------------------\n",
    "\n",
    "    ## Printing a summary of the layers and weights in your model\n",
    "#     model.summary()\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#     fit = model.fit(np.array(X_train), np.array(y_train), batch_size=150, nb_epoch=10,\n",
    "#        verbose=1)\n",
    "\n",
    "#     ## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "#     score = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "#     print('Test score:', score[0])\n",
    "#     print('Test accuracy:', score[1])\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    models.append(model)\n",
    "\n",
    "#     test.fit(X_train, y_train)\n",
    "    \n",
    "#     pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    false_pos_rate, true_pos_rate, thresholds = metrics.roc_curve(y_test, pred, pos_label=1)\n",
    "    print(metrics.auc(false_pos_rate, true_pos_rate))\n",
    "    aucs.append(metrics.auc(false_pos_rate, true_pos_rate))\n",
    "    \n",
    "max_auc = max(aucs)\n",
    "best_index = aucs.index(max_auc)\n",
    "best_model = models[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2008 predictions\n",
    "predictions = best_model.predict(test_data_08).flatten()\n",
    "with open('test_2008_predictions.csv', mode='w') as test_2008_predictions:\n",
    "    prediction_writer = csv.writer(test_2008_predictions, delimiter=',')\n",
    "    prediction_writer.writerow(['id', 'target'])\n",
    "    for row in range(len(predictions)):\n",
    "        prediction_writer.writerow([row, predictions[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012 predictions\n",
    "predictions = best_model.predict(test_data_12).flatten()\n",
    "with open('test_2012_predictions.csv', mode='w') as test_2012_predictions:\n",
    "    prediction_writer = csv.writer(test_2012_predictions, delimiter=',')\n",
    "    prediction_writer.writerow(['id', 'target'])\n",
    "    for row in range(len(predictions)):\n",
    "        prediction_writer.writerow([row, predictions[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
