{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras import optimizers\n",
    "from sklearn.svm import SVC\n",
    "from keras.layers import BatchNormalization\n",
    "import csv\n",
    "\n",
    "# Load libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Import Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97697\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "training_data = load_data('train_2008.csv', 1)\n",
    "\n",
    "# oversample the ones\n",
    "nonvoters = training_data[np.where(training_data[:,-1] == 0)]\n",
    "voters = training_data[np.where(training_data[:,-1] == 1)]\n",
    "\n",
    "# indices_of_nonvoters_sample = np.random.choice(len(nonvoters), len(voters))\n",
    "\n",
    "# recompile the training data\n",
    "training_data = np.concatenate((np.concatenate((voters, np.concatenate((voters, voters), axis=0)), axis=0), nonvoters), axis=0)\n",
    "print(len(training_data))\n",
    "np.random.shuffle(training_data)\n",
    "\n",
    "# get all of the parameters\n",
    "train_data = training_data[:,:-1]\n",
    "# get all the targets\n",
    "train_target = training_data[:, -1]\n",
    "\n",
    "# load testing data\n",
    "test_data_08 = load_data('test_2008.csv', 1)\n",
    "test_data_12 = load_data('test_2012.csv', 1)\n",
    "\n",
    "# find any column which contains only one value in all its rows (such as interview month/year)\n",
    "useless_columns = np.all(train_data == train_data[0,:], axis=0)\n",
    "indices_to_remove = []\n",
    "for i in range(len(useless_columns)):\n",
    "    if useless_columns[i]:\n",
    "        indices_to_remove.append(i)\n",
    "        \n",
    "#delete those columns\n",
    "train_data = np.delete(train_data, indices_to_remove, 1)\n",
    "test_data_08 = np.delete(test_data_08, indices_to_remove, 1)\n",
    "test_data_12 = np.delete(test_data_12, indices_to_remove, 1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data_08 = scaler.transform(test_data_08)\n",
    "test_data_12 = scaler.transform(test_data_12)\n",
    "\n",
    "# train_target = np.reshape(training_data[:, -1], (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top k features\n",
    "num_params = 15\n",
    "test = SelectKBest(k=num_params)\n",
    "fit = test.fit(train_data, train_target)\n",
    "\n",
    "scores = fit.scores_.tolist()\n",
    "scores_copy = fit.scores_.tolist()\n",
    "best_scores = []\n",
    "best_indices = []\n",
    "\n",
    "for i in range(num_params):\n",
    "    m = max(scores)\n",
    "    best_scores.append(m)\n",
    "    best_indices.append(scores_copy.index(m))\n",
    "    scores.remove(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97697\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data[:, best_indices]\n",
    "print(len(train_data))\n",
    "test_data_08 = test_data_08[:, best_indices]\n",
    "test_data_12 = test_data_12[:, best_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(600, input_shape=(len(best_indices),), kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "#     history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=150)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97697\n",
      "Epoch 1/10\n",
      "78157/78157 [==============================] - 12s 152us/step - loss: 0.5954\n",
      "Epoch 2/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5919\n",
      "Epoch 3/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5906\n",
      "Epoch 4/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5905\n",
      "Epoch 5/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5894\n",
      "Epoch 6/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5884\n",
      "Epoch 7/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5886\n",
      "Epoch 8/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5883\n",
      "Epoch 9/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5882\n",
      "Epoch 10/10\n",
      "78157/78157 [==============================] - 11s 140us/step - loss: 0.5880\n",
      "0.7632847161598038\n",
      "Epoch 1/10\n",
      "78157/78157 [==============================] - 12s 154us/step - loss: 0.5949\n",
      "Epoch 2/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5915\n",
      "Epoch 3/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5908\n",
      "Epoch 4/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5897\n",
      "Epoch 5/10\n",
      "78157/78157 [==============================] - 11s 143us/step - loss: 0.5887\n",
      "Epoch 6/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5884\n",
      "Epoch 7/10\n",
      "78157/78157 [==============================] - 11s 142us/step - loss: 0.5880\n",
      "Epoch 8/10\n",
      "78157/78157 [==============================] - 11s 137us/step - loss: 0.5874\n",
      "Epoch 9/10\n",
      "78157/78157 [==============================] - 10s 125us/step - loss: 0.5875\n",
      "Epoch 10/10\n",
      "78157/78157 [==============================] - 10s 129us/step - loss: 0.5877\n",
      "0.7605731276580702\n",
      "Epoch 1/10\n",
      "78158/78158 [==============================] - 11s 143us/step - loss: 0.5947\n",
      "Epoch 2/10\n",
      "78158/78158 [==============================] - 10s 131us/step - loss: 0.5906\n",
      "Epoch 3/10\n",
      "78158/78158 [==============================] - 10s 130us/step - loss: 0.5900\n",
      "Epoch 4/10\n",
      "78158/78158 [==============================] - 10s 129us/step - loss: 0.5886\n",
      "Epoch 5/10\n",
      "78158/78158 [==============================] - 10s 132us/step - loss: 0.5886\n",
      "Epoch 6/10\n",
      "78158/78158 [==============================] - 11s 137us/step - loss: 0.5877\n",
      "Epoch 7/10\n",
      "78158/78158 [==============================] - 11s 143us/step - loss: 0.5872\n",
      "Epoch 8/10\n",
      "78158/78158 [==============================] - 11s 143us/step - loss: 0.5875\n",
      "Epoch 9/10\n",
      "78158/78158 [==============================] - 11s 143us/step - loss: 0.5865\n",
      "Epoch 10/10\n",
      "78158/78158 [==============================] - 11s 143us/step - loss: 0.5862\n",
      "0.7583956344290386\n",
      "Epoch 1/10\n",
      "78158/78158 [==============================] - 12s 157us/step - loss: 0.5934\n",
      "Epoch 2/10\n",
      "78158/78158 [==============================] - 11s 144us/step - loss: 0.5903\n",
      "Epoch 3/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5895\n",
      "Epoch 4/10\n",
      "78158/78158 [==============================] - 11s 144us/step - loss: 0.5877\n",
      "Epoch 5/10\n",
      "78158/78158 [==============================] - 11s 144us/step - loss: 0.5878\n",
      "Epoch 6/10\n",
      "78158/78158 [==============================] - 11s 146us/step - loss: 0.5869\n",
      "Epoch 7/10\n",
      "78158/78158 [==============================] - 11s 146us/step - loss: 0.5871\n",
      "Epoch 8/10\n",
      "78158/78158 [==============================] - 11s 146us/step - loss: 0.5871\n",
      "Epoch 9/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5862\n",
      "Epoch 10/10\n",
      "78158/78158 [==============================] - 11s 144us/step - loss: 0.5865\n",
      "0.754459905320064\n",
      "Epoch 1/10\n",
      "78158/78158 [==============================] - 13s 161us/step - loss: 0.5948\n",
      "Epoch 2/10\n",
      "78158/78158 [==============================] - 11s 138us/step - loss: 0.5913\n",
      "Epoch 3/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5904\n",
      "Epoch 4/10\n",
      "78158/78158 [==============================] - 11s 146us/step - loss: 0.5898\n",
      "Epoch 5/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5889\n",
      "Epoch 6/10\n",
      "78158/78158 [==============================] - 11s 146us/step - loss: 0.5886\n",
      "Epoch 7/10\n",
      "78158/78158 [==============================] - 12s 148us/step - loss: 0.5887\n",
      "Epoch 8/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5877\n",
      "Epoch 9/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5875\n",
      "Epoch 10/10\n",
      "78158/78158 [==============================] - 11s 145us/step - loss: 0.5868\n",
      "0.7610381467557894\n",
      "97697\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "\n",
    "\n",
    "# 5-fold validation\n",
    "kf = KFold(n_splits=5)\n",
    "models = []\n",
    "aucs = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_data):\n",
    "    X_train, y_train = train_data[train_index], train_target[train_index]\n",
    "    X_test, y_test = train_data[test_index], train_target[test_index]\n",
    "    \n",
    "    # test = tree.DecisionTreeClassifier(criterion='gini')\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "#     clf = SVC(gamma='auto')\n",
    "#     clf.fit(X_train, y_train) \n",
    "    \n",
    "#     pred = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "#     nn_estimator = KerasRegressor(build_fn= neural_net_model, validation_split=0.2, epochs=10, batch_size=150)\n",
    "    \n",
    "#     boosted_nn = AdaBoostRegressor(base_estimator = nn_estimator)\n",
    "#     model = boosted_nn.fit(X_train, y_train)\n",
    "#     pred = boosted_nn.predict(X_test)\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "#     svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "#     # Create adaboost classifer object\n",
    "#     abc =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)\n",
    "\n",
    "#     # Train Adaboost Classifer\n",
    "#     model = abc.fit(X_train, y_train)\n",
    "\n",
    "#     #Predict the response for test dataset\n",
    "#     pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#     # Model Accuracy, how often is the classifier correct?\n",
    "#     print(\"Accuracy:\",metrics.accuracy_score(y_test, pred))\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_shape=(len(best_indices),), kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(480))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "        \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    models.append(model)\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    \n",
    "#     adam = optimizers.adam(lr = 0.005, decay = 0.0000001)\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(480, input_dim=X_train.shape[1],\n",
    "#                     kernel_initializer='normal',\n",
    "#                     #kernel_regularizer=regularizers.l2(0.02),\n",
    "#                     activation=\"relu\"))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(240,\n",
    "#                     #kernel_regularizer=regularizers.l2(0.02),\n",
    "#                     activation=\"tanh\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "#     #--------------------------------------------\n",
    "    \n",
    "    ## Printing a summary of the layers and weights in your model\n",
    "#     model.summary()\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#     fit = model.fit(np.array(X_train), np.array(y_train), batch_size=150, nb_epoch=10,\n",
    "#        verbose=1)\n",
    "\n",
    "#     ## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "#     score = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "#     print('Test score:', score[0])\n",
    "#     print('Test accuracy:', score[1])\n",
    "\n",
    "#     test.fit(X_train, y_train)\n",
    "    \n",
    "#     pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    false_pos_rate, true_pos_rate, thresholds = metrics.roc_curve(y_test, pred, pos_label=1)\n",
    "    print(metrics.auc(false_pos_rate, true_pos_rate))\n",
    "    aucs.append(metrics.auc(false_pos_rate, true_pos_rate))\n",
    "    \n",
    "print(len(train_data))\n",
    "max_auc = max(aucs)\n",
    "best_index = aucs.index(max_auc)\n",
    "best_model = models[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2008 predictions\n",
    "predictions = best_model.predict(test_data_08).flatten()\n",
    "with open('test_2008_predictions.csv', mode='w') as test_2008_predictions:\n",
    "    prediction_writer = csv.writer(test_2008_predictions, delimiter=',')\n",
    "    prediction_writer.writerow(['id', 'target'])\n",
    "    for row in range(len(predictions)):\n",
    "        prediction_writer.writerow([row, predictions[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012 predictions\n",
    "predictions = best_model.predict(test_data_12).flatten()\n",
    "with open('test_2012_predictions.csv', mode='w') as test_2012_predictions:\n",
    "    prediction_writer = csv.writer(test_2012_predictions, delimiter=',')\n",
    "    prediction_writer.writerow(['id', 'target'])\n",
    "    for row in range(len(predictions)):\n",
    "        prediction_writer.writerow([row, predictions[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
